{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerを用いた機械学習の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを含む多くの機械学習における学習タスクは`最適なパラメータを探す問題`です。\n",
    "最適なパラメータは目的関数の最小化（最大化）問題を解くことで自動的に得られます。\n",
    "\n",
    "次のステップによって、自分の白州モデルを作っていくこととなります。\n",
    "\n",
    "1. モデルの定義をする\n",
    "2. 目的関数を定義する\n",
    "3. 目的関数を最適化することで，モデルを学習させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義をする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はじめに学習対象のモデルを定義します。\n",
    "\n",
    "ここでは，学習対象のモデルはいくつかのパラメータを使った関数だとします。\n",
    "Pythonプログラムでいうと，学習対象のモデルはクラスのメソッドであり，パラメータはメンバ変数（インスタンス）のようなものです。\n",
    "\n",
    "例えば次のクラスlayerはパラメータ $a$ と $b$ を持ち，関数としては $ax + b$を表します。<br>\n",
    "この関数の挙動はパラメータ $a$ と $b$ を変えることで変わります。\n",
    "\n",
    "```python\n",
    "class layer(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.a * x + self.b\n",
    "\n",
    "l = layer(2.0, -1.0)\n",
    "print(l(1.0)) # 1.0\n",
    "print(l(2.0)) # 3.0\n",
    "```\n",
    "\n",
    "\n",
    "同様に，学習対象のモデルも複数のパラメータを持ち，それらパラメータを調整することで望むような挙動をするようにさせます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 層について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 練習<br>\n",
    "\n",
    "与える、引数を変えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "class layer(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.a * x + self.b\n",
    "\n",
    "\n",
    "l = layer(2.0, -1.0)\n",
    "print(l(5.0))  \n",
    "print(l(8.0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerでは学習可能なモデルをLinkとよびます。<br>\n",
    "\n",
    "ディープラーニングで利用される代表的なLinkは `chainer.links` でサポートされています。<br>\n",
    "また，自分で新しいLinkを作ることもできます。<br>\n",
    "<br>\n",
    "以降では，この `chainer.links` を `L` として使えるようにします。<br>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "from chainer import links as L\n",
    "```\n",
    "<br>\n",
    "もっと基本的なLinkはLinearとよばれるLinkです。<br>\n",
    "Linearは全ての入力と出力がつながっているようなニューラルネットワークを表します。<br>\n",
    "Linearはニューラルネットワークの文脈では全結合層，数学の用語では線形変換，アフィン変換とよびます。<br>\n",
    "たとえば，次の例では5個のユニットから，2個のユニットへの変換を表します。<br>\n",
    "<br>\n",
    "```python\n",
    "lin = L.Linear(5, 2)\n",
    "```\n",
    "<br>\n",
    "この `lin` はLinkオブジェクトですが，次のように関数呼び出しをすることができます。<br>\n",
    "（この関数呼び出しは `__call__` で定義されており，演算子オーバーロードで実現されています。）<br>\n",
    "<br>\n",
    "```python\n",
    "import numpy as np\n",
    "from chainer import Variable\n",
    "\n",
    "lin = L.Linear(5, 2)\n",
    "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
    "y1 = lin(x)\n",
    "```\n",
    "<br>\n",
    "この `numpy` ， `Variable` については後で詳しく説明します。<br>\n",
    "この例である`np.ones((3, 5), dtype=np.float32)` は3行5列で全ての値が1であるような行列を作ります。<br>\n",
    "`Variable` はその値に加えて学習に必要な情報が埋め込まれているオブジェクトとだけ覚えてください。<br>\n",
    "つまりここでは3個の5次元のベクトルを用意し，それをVariableというオブジェクトにセットし，<br>\n",
    "それを `lin` の引数として与えて，出力を `y` として計算しています。<br>\n",
    "`lin`は5次元の入力を2次元の出力へ変換する関数なので，`y`は3個の2次元のベクトルになります。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 線形変換，アフィン変換 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形変換（アフィン変換）は次のように表される変換です。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x; θ) &= Wx + b \\\\\n",
    "θ &= (W, b)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "例えば上記例のLinearは，5次元のベクトルから2次元のベクトルへの線形変換を表します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 練習\n",
    "\n",
    "上記の例を用いて，出力次元を4次元にして出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]])\n",
      "variable([[ 0.30907738 -0.9319836  -0.08234261  0.20045832]\n",
      "          [ 0.30907738 -0.9319836  -0.08234261  0.20045832]\n",
      "          [ 0.30907738 -0.9319836  -0.08234261  0.20045832]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "\n",
    "link = L.Linear(5, 4)\n",
    "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
    "y1 = link(x)\n",
    "\n",
    "print(x)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerでもう一つ重要なオブジェクトとしてFunctionがあります。<br>\n",
    "FuncitonはLinkとは違って，学習可能なパラメータを持ちません。<br>\n",
    "つまり，学習によって挙動を変えません。<br>\n",
    "<br>\n",
    "ディープラーニングで利用されている代表的な関数は `chainer.functions` で定義されています。<br>\n",
    "また，自分で新しいFunctionを作ることもできます。<br>\n",
    "<br>\n",
    "以降では，この  `chainer.functions` をFとして使えるようにします。<br>\n",
    "<br>\n",
    "```python\n",
    "from chainer import functions as F\n",
    "```\n",
    "<br>\n",
    "例えば，ディープラーニングでよく使われるReLUとよばれる非線形関数 $f_{relu}$ は<br>\n",
    "<br>\n",
    "$$f_{relu}(x)=max(x,0)$$\n",
    "<br>\n",
    "で定義されます。<br>\n",
    "つまり，もし $x$ が $0$ よりも大きければ $x$ をそのまま返し，もし小さければ $0$ を返すような関数です。<br>\n",
    "<br>\n",
    "Chainerでは次のように記述できます。<Br>\n",
    "<br>\n",
    "```python\n",
    "from chainer import functions as F\n",
    "...\n",
    "y2 = F.relu(x)\n",
    "```\n",
    "<br>\n",
    "これらのLinkとFunctionを組みわせて複雑な関数を作ることができます。<br>\n",
    "例えば，前回の例のLinearを適用した後にReLUを適用した結果は次のように計算されます。<br>\n",
    "<br>\n",
    "```\n",
    "y3 = F.relu(lin(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 練習\n",
    "\n",
    "先ほどのy1をReLU関数及び、もう一つの有名な関数`sigmoid関数`に与えてみましょう。<br>\n",
    "<br>\n",
    "・sigmoid\n",
    "<br>\n",
    "$$f_{sigmoid}(x)=\\frac{1}{1+exp(-x)}$$\n",
    "<br>\n",
    "```\n",
    "y3 = F.sigmoid(link(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.30907738 0.         0.         0.20045832]\n",
      "          [0.30907738 0.         0.         0.20045832]\n",
      "          [0.30907738 0.         0.         0.20045832]])\n",
      "variable([[0.57666004 0.28252244 0.47942597 0.54994744]\n",
      "          [0.57666004 0.28252244 0.47942597 0.54994744]\n",
      "          [0.57666004 0.28252244 0.47942597 0.54994744]])\n"
     ]
    }
   ],
   "source": [
    "from chainer import functions as F\n",
    "\n",
    "y2 = F.relu(y1)\n",
    "y3 = F.sigmoid(y1)\n",
    "\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで扱ったLinkとFunctionを組み合わせて，学習対象のモデルを実際に作ってみましょう。<br>\n",
    "<br>\n",
    "以下に三層からなるニューラルネットワークの例をあげます。<br>\n",
    "<br>\n",
    "```python\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "```\n",
    "<br>\n",
    "各機能は今後詳細に説明されますので，ここでは概要だけ説明します。<br>\n",
    "詳細は理解できなくてもそのまま飛ばして問題ありません。<br>\n",
    "<br>\n",
    "このMLPは，三つのLinear（l1, l2, l3）を学習可能なパラメータとして持ち，<br>\n",
    "`__call__`でそれらのパラメータを利用して結果を計算します。<br>\n",
    "<br>\n",
    "なお，`L.Linear` の第一引数には `None` を指定することで実際の入力からユニット数を自動で設定してくれます。<br>\n",
    "<br>\n",
    "`__call__` では先ほど定義した層に入力`x`を与えて計算（順計算）を行います。<br>\n",
    "まず `l1` に大元の入力 `x` を与え，それをLinearで変換したものにReLUを適用します。<br>\n",
    "その計算結果 `h1` を次の層 `l2` に与え同様の計算を行います。<br>\n",
    "`l3` に関しても同様に前層の結果を元に計算を行います。<br>\n",
    "最終的に `l3` の結果を返すことで計算が完了します。<br>\n",
    "<br>\n",
    "このように<br>\n",
    "(1)Linkを使って学習対象のパラメータを定義<br>\n",
    "(2)次にそれらを使って順計算を定義<br>\n",
    "することで学習対象のモデルを定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import links as L\n",
    "from chainer import functions as F\n",
    "from chainer import Chain\n",
    "\n",
    "from chainer import Variable\n",
    "\n",
    "class MLP(Chain):\n",
    "    def __init__(self, n_unit, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_unit)\n",
    "            self.l2 = L.Linear(None, n_unit)\n",
    "            self.l3 = L.Linear(None, n_out)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(5, 2)\n",
    "x = Variable(np.ones((4, 5), dtype=np.float32))\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数は一つの値を出力し，値が小さければ望ましい状態を表すような関数です。<br>\n",
    "<br>\n",
    "例えば，訓練データに対し学習対象モデルが予測をし，間違えた回数を $L$ とします。<br>\n",
    "\n",
    "この間違えた回数 $L$ を小さくするということは，学習対象モデルが訓練データをたくさん当てられるようにすることを意味します。<br>\n",
    "\n",
    "この場合，目的関数は学習対象モデルを引数として受取り，間違えた数を返すような関数です。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力層における関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回，学習したいモデルは入力 $x$ が与えられた時に、<br>\n",
    "出力 $y$ の条件付き確率 $p(y \\mid x)$ を出力してくれるようなモデルです。<br>\n",
    "<br>\n",
    "例えば，入力$x$に犬が写っていれば $p(犬 \\mid x) = 0.99, p(猫 \\mid x) = 0.01$ となるようなモデルです。\n",
    "<br>\n",
    "このようなカテゴリ値（離散値）に対する確率分布をモデル化するにはSoftmaxを利用します。<br>\n",
    "<br>\n",
    "Chainerでは `chainer.functions` で `softmax` 関数が定義されているのでそれを使いましょう。<br>\n",
    "<br>\n",
    "例えば，入力`x`を何らかのモデルで変換しカテゴリ種類数と同じ次元数を持つベクトル`t`を作ります。<br>\n",
    "次に（必ずしも確率分布となっていない）ベクトル`t`を`softmax`を使って確率分布に変換します。<br>\n",
    "<br>\n",
    "```python\n",
    "t = model(x)\n",
    "y = F.softmax(t)\n",
    "```\n",
    "<br>\n",
    "- Softmax\n",
    "<br>\n",
    "Softmax（または多クラスロジスティックスモデル）とは $d$ 次元の実数値ベクトルから，<br>\n",
    "$d$ 次元の確率分布を作る方法の一つです。<br>\n",
    "<br>\n",
    "softmaxは，$x$ が $d$ 次元であり，各次元の値が $x[0], x[1], ..., x[d-1]$ の時，<br>\n",
    "<br>\n",
    "$$y[i]=\\frac{\\exp(x[i])}{\\sum_j \\exp(x[j])}$$\n",
    "<br>\n",
    "と表されます。<br>\n",
    "<br>\n",
    "\n",
    "あるベクトルvが確率分布となる条件として，<br>\n",
    "\n",
    "1. 各次元の値が非負\n",
    "2. 合計値が1\n",
    "\n",
    "という条件があります。<br>\n",
    "Softmaxは， $\\exp$ が非負であることから1の条件を満たし，<br>\n",
    "各次元の値を足した値で割っていることから2の条件を満たします。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- クロスエントロピー損失関数\n",
    "\n",
    "学習の目標は学習データ$D$の確率分布 $p(y|x)$ と，\n",
    "学習対象のモデルによる確率分布 $q(y \\mid x; \\theta)$ が一致するようにすることです。<br>\n",
    "確率分布間がどれだけ離れているかを表す指標としてKLダイバージェンスが知られています。<br>\n",
    "KLダイバージェンスは二つの確率分布 $P$ と $Q$ の遠さを次のように定義します。<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(P \\mid\\mid Q) &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\\\\n",
    "                 &= \\sum_x P(x) \\log P(x) - \\sum_x P(x) \\log Q(x)\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "もし， $P$ と $Q$ が同じならば，全ての $x$ について $\\frac{P(x)}{Q(x)}=1$ となるので \n",
    "$KL(P \\mid\\mid Q)=0$ となります。<br>\n",
    "この二つの分布が違うと， $KL(P \\mid\\mid Q)>0$ となり，近ければ近いほど0に近づくような指標です。<br>\n",
    "<br>\n",
    "学習データによって定義される確率分布は訓練分布と呼ばれ，それに基づく条件付き確率は<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y \\mid x) &= \\frac{P(x, y)}{P(x)} \\\\\n",
    "            &= \\frac{\\sum_{i}I(x=x_i, y=y_i)}{\\sum_{i}I(x=x_i)}\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "と表されます。<br>\n",
    "但し， $I$ はデルタ関数とよばれ， $I(c)$ は $c$ が真である時は $1$，それ以外は $0$ であるような関数です。<br>\n",
    "<br>\n",
    "\n",
    "訓練分布に基づく条件づき確率 $P(y \\mid x)$ と，モデルによる条件づき確率 $Q(y \\mid x)$ のKLダイバージェンスは<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "KL(P \\mid\\mid Q) &= \\sum_{x} \\sum_y P(y \\mid x) \\log \\frac{P(y \\mid x)}{Q(y \\mid x)} \\\\\n",
    "                 &= \\sum_{x, y} P(y \\mid x) \\log P(y \\mid x) - \\sum_{x, y} P(y \\mid x) \\log Q(y \\mid x)\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "となります。<br>\n",
    "この最適化において， $Q$ に依存する項は第二項のみであり， $(x, y) \\in D$ の時 $P(y \\mid x)=1$ ，それ以外 $0$ ですので<br>\n",
    "<br>\n",
    "$L(\\theta) = - \\sum_{i=1}^n \\log Q(y_i \\mid x_i)$\n",
    "<br>\n",
    "となります。<br>\n",
    "これをクロスエントロピー損失関数，または負の対数尤度ともよばれます。<br>\n",
    "<br>\n",
    "ここまで読んだ方で，なぜ学習データ $D$ から得られた確率分布 $P$ そのものを直接使わず，\n",
    "学習モデルによる確率 $Q$ を使うのかと思った方がいるかもしれません。<br>\n",
    "それは，学習の目標は学習データだけをうまく分類することではなく未知のデータをうまく分類することだからです。<br>\n",
    "<br>\n",
    "$P$ は，入力が学習データと全く同じであればそれが正解となりますが，そうでない場合は確率分布が不定になりえます。<br>\n",
    "$Q$ はありえそうなモデルの中で一番 $P$ に近い分布を探しています。<br>\n",
    "$Q$ は $P$ とは違って全ての $x$ について確率分布を与えることができるため，<br>\n",
    "学習データには含まれない未知のデータでもうまく分類できます。<br>\n",
    "別の言い方をすると$P$を平滑化した確率分布が$Q$となります。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行うために，入力と正解の出力のペアからなる $n$ 個の学習データ\n",
    "<br>\n",
    "$$D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$$\n",
    "<br>\n",
    "を用意します。<br>\n",
    "<br>\n",
    "この学習データと，学習対象のモデルが一致するように，つまり学習データそれぞれ $(x_i, y_i)$ に対し， <br>\n",
    "$p(y_i \\mid x_i)$ が大きい時に，値が小さくなるような目的関数を用意します。<br>\n",
    "<br>\n",
    "ここでは代表的な関数である`softmax_cross_entropy`とよばれる関数を使います。<br>\n",
    "<br>\n",
    "```python\n",
    "## x は入力, tは正解の出力\n",
    "h = MLP(x)\n",
    "loss = F.softmax_cross_entropy(h, t)\n",
    "```\n",
    "<br>\n",
    "ChainerではSoftmaxを適用した後に，クロスエントロピー損失関数を適用した目的関数が用意されています。<br>\n",
    "これは，二つまとめて処理をしないと，数値誤差の問題があるためです。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "581px",
    "left": "0px",
    "right": "989px",
    "top": "131px",
    "width": "291px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
